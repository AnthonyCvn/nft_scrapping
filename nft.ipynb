{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining for NFTs\n",
    "\n",
    "- Scrap and store data from rarity tool website about trendy NFTs\n",
    "- Find influencers in NFT and spot if they speak about a particular NFT\n",
    "- Store information about Google trends\n",
    "\n",
    "Idea for implementation\n",
    "- When was the Twitter account created\n",
    "- sentiment analysis on Twitts\n",
    "- Volume\n",
    "- Launch date of the NFT\n",
    "- ETH value (if low buy more easily)\n",
    "\n",
    "Packages that need to be installed\n",
    "- !pip install pytrends\n",
    "- !pip3 install snscrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Progress bar library\n",
    "from tqdm import tqdm\n",
    "\n",
    "# We will use snscrape from github.com/JustAnotherArchivist/snscrape to get data from Twitter\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "from datetime import date, timedelta\n",
    "\n",
    "# We use pytrend to fetch information from Google trend\n",
    "from pytrends.request import TrendReq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrap and store data from rarity tool website about trendy NFTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import selenium webdriver and configure its options\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "options = Options()\n",
    "options.headless = False\n",
    "options.add_argument(\"--window-size=1920,1200\")\n",
    "\n",
    "s=Service('./chromedriver')\n",
    "\n",
    "driver = webdriver.Chrome(options=options, service=s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get driver for rarity tool website\n",
    "driver.get('https://rarity.tools/upcoming')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rarity tool page is ready!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4t/5wd2c1y903ncr7r3dmlwskrr0000gn/T/ipykernel_40708/220282747.py:10: DeprecationWarning: find_elements_by_* commands are deprecated. Please use find_elements() instead\n",
      "  url_elements = driver.find_elements_by_xpath(\"//*[contains(@class,'text-left text-gray-800')]\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Doodle Ape Champs',\n",
       " '2D Ape themed NFT avatars',\n",
       " 'Discord',\n",
       " '@doodleapechamps',\n",
       " 'www.doodleapechamps.art',\n",
       " '0.5 SOL',\n",
       " '250 Total',\n",
       " '6:00 pm (Europe/Berlin)',\n",
       " 'Yesterday']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wait until page is loaded\n",
    "delay = 10 # seconds\n",
    "try:\n",
    "    myElem = WebDriverWait(driver, delay).until(EC.presence_of_element_located((By.XPATH, \"//*[contains(@class,'text-left text-gray-800')]\")))\n",
    "    print(\"Rarity tool page is ready!\")\n",
    "except TimeoutException:\n",
    "    print(\"Loading took longer than {0} seconds\".format(delay))\n",
    "\n",
    "# Find NFT elements\n",
    "url_elements = driver.find_elements_by_xpath(\"//*[contains(@class,'text-left text-gray-800')]\")\n",
    "\n",
    "# Show first NFT\n",
    "url_elements[0].text.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with NFTs from rarity tool\n",
    "nft_list = []\n",
    "for element in url_elements:\n",
    "    nft_list.append(element.text.split('\\n'))\n",
    "\n",
    "df = pd.DataFrame(nft_list)\n",
    "\n",
    "# Close Selenium driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the name, description, website, volume, and twitter account\n",
    "df['Name'] = df[0]\n",
    "df['Description'] = df[1]\n",
    "df['Website'] = df[4]\n",
    "df['Twitter Profile'] = df[3].apply(lambda x: x[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '0.0650.09'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 7\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrice ETH\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcleanPrice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch_m1/lib/python3.8/site-packages/pandas/core/series.py:4430\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4320\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4321\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4322\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4325\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4326\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4327\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4328\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4329\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4428\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4429\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4430\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch_m1/lib/python3.8/site-packages/pandas/core/apply.py:1082\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1079\u001b[0m     \u001b[38;5;66;03m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[1;32m   1080\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m-> 1082\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch_m1/lib/python3.8/site-packages/pandas/core/apply.py:1137\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m   1132\u001b[0m         \u001b[38;5;66;03m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m         \u001b[38;5;66;03m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;66;03m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[0;32m-> 1137\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1140\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1144\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1145\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch_m1/lib/python3.8/site-packages/pandas/_libs/lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mcleanPrice\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcleanPrice\u001b[39m(x):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meth\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mlower():\n\u001b[0;32m----> 4\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misnumeric\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '0.0650.09'"
     ]
    }
   ],
   "source": [
    "# Clean the price tag\n",
    "def cleanPrice(x):\n",
    "    if 'eth' in x.lower():\n",
    "        return float(''.join([i for i in x if i.isnumeric() or i=='.']))\n",
    "    else:\n",
    "        return 0\n",
    "df['Price ETH'] = df[5].apply(cleanPrice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data on each Twitter profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/202 [00:00<?, ?it/s]Error retrieving https://api.twitter.com/2/timeline/profile/1482304140265295880.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_ext_alt_text=true&include_quote_count=true&include_reply_count=1&tweet_mode=extended&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&send_error_codes=true&simple_quoted_tweets=true&include_tweet_replies=true&userId=1482304140265295880&count=100&ext=mediaStats%2ChighlightedLabel: blocked (429)\n",
      "4 requests to https://api.twitter.com/2/timeline/profile/1482304140265295880.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_ext_alt_text=true&include_quote_count=true&include_reply_count=1&tweet_mode=extended&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&send_error_codes=true&simple_quoted_tweets=true&include_tweet_replies=true&userId=1482304140265295880&count=100&ext=mediaStats%2ChighlightedLabel failed, giving up.\n",
      "  0%|                                                   | 0/202 [00:09<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ScraperException",
     "evalue": "4 requests to https://api.twitter.com/2/timeline/profile/1482304140265295880.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_ext_alt_text=true&include_quote_count=true&include_reply_count=1&tweet_mode=extended&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&send_error_codes=true&simple_quoted_tweets=true&include_tweet_replies=true&userId=1482304140265295880&count=100&ext=mediaStats%2ChighlightedLabel failed, giving up.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mScraperException\u001b[0m                          Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sntwitter\u001b[38;5;241m.\u001b[39mTwitterUserScraper\u001b[38;5;241m.\u001b[39mis_valid_username(twitter_name):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 8\u001b[0m         items \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msntwitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTwitterProfileScraper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{0}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtwitter_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43misUserId\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_items\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTwitter profile \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m not found\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(twitter_name))\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch_m1/lib/python3.8/site-packages/snscrape/modules/twitter.py:813\u001b[0m, in \u001b[0;36mTwitterProfileScraper.get_items\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    810\u001b[0m params \u001b[38;5;241m=\u001b[39m paginationParams\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    811\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcursor\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 813\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_api_data(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://api.twitter.com/2/timeline/profile/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00muserId\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m, params, paginationParams):\n\u001b[1;32m    814\u001b[0m \t\u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_instructions_to_tweets(obj)\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch_m1/lib/python3.8/site-packages/snscrape/modules/twitter.py:369\u001b[0m, in \u001b[0;36m_TwitterAPIScraper._iter_api_data\u001b[0;34m(self, endpoint, params, paginationParams, cursor, direction)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    368\u001b[0m \t_logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRetrieving scroll page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcursor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 369\u001b[0m \tobj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_api_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreqParams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    370\u001b[0m \t\u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m    372\u001b[0m \t\u001b[38;5;66;03m# No data format test, just a hard and loud crash if anything's wrong :-)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch_m1/lib/python3.8/site-packages/snscrape/modules/twitter.py:339\u001b[0m, in \u001b[0;36m_TwitterAPIScraper._get_api_data\u001b[0;34m(self, endpoint, params)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_api_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, endpoint, params):\n\u001b[1;32m    338\u001b[0m \t\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_guest_token()\n\u001b[0;32m--> 339\u001b[0m \tr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apiHeaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponseOkCallback\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_api_response\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m \t\u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    341\u001b[0m \t\tobj \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch_m1/lib/python3.8/site-packages/snscrape/base.py:216\u001b[0m, in \u001b[0;36mScraper._get\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 216\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch_m1/lib/python3.8/site-packages/snscrape/base.py:212\u001b[0m, in \u001b[0;36mScraper._request\u001b[0;34m(self, method, url, params, data, headers, timeout, responseOkCallback, allowRedirects)\u001b[0m\n\u001b[1;32m    210\u001b[0m \tmsg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retries \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requests to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreq\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m failed, giving up.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    211\u001b[0m \tlogger\u001b[38;5;241m.\u001b[39mfatal(msg)\n\u001b[0;32m--> 212\u001b[0m \t\u001b[38;5;28;01mraise\u001b[39;00m ScraperException(msg)\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReached unreachable code\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mScraperException\u001b[0m: 4 requests to https://api.twitter.com/2/timeline/profile/1482304140265295880.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_ext_alt_text=true&include_quote_count=true&include_reply_count=1&tweet_mode=extended&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&send_error_codes=true&simple_quoted_tweets=true&include_tweet_replies=true&userId=1482304140265295880&count=100&ext=mediaStats%2ChighlightedLabel failed, giving up."
     ]
    }
   ],
   "source": [
    "# Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "profile_list = []\n",
    "\n",
    "# Use TwitterSearchScraper to scrape data for each NFT\n",
    "for twitter_name in tqdm(df['Twitter Profile']):\n",
    "    if sntwitter.TwitterUserScraper.is_valid_username(twitter_name):\n",
    "        try:\n",
    "            items = next(sntwitter.TwitterProfileScraper('{0}'.format(twitter_name), isUserId=False).get_items())\n",
    "        except AttributeError:\n",
    "            print('Twitter profile {0} not found'.format(twitter_name))\n",
    "        else:  \n",
    "            profile_list.append([twitter_name, items.user.created, items.user.description, \n",
    "                                 items.user.favouritesCount, items.user.followersCount, \n",
    "                                 items.user.friendsCount, items.user.listedCount, items.user.statusesCount, \n",
    "                                 items.user.mediaCount, items.user.location, items.user.profileBannerUrl, \n",
    "                                 items.user.profileImageUrl])\n",
    "    else:\n",
    "        print('Twitter profile {0} is not a valid username'.format(twitter_name))\n",
    "\n",
    "# Creating a dataframe from the tweets list above\n",
    "profiles_df = pd.DataFrame(profile_list, columns=['Twitter Profile','Twitter User Created', 'Twitter User Description', 'Twitter User Favourites Count', \n",
    "                                                  'Twitter User Followers Count', 'Twitter User Friends Count', 'Twitter User Listed Count', \n",
    "                                                  'Twitter User Statuses Count', 'Twitter User Media Count', 'Twitter User Location', \n",
    "                                                  'Twitter User Profile Banner Url', 'Twitter User Profile Image Url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge and save dataframe\n",
    "nft_df = df[['Twitter Profile', 'Name', 'Description', 'Website', 'Price ETH']].merge(profiles_df, on='Twitter Profile', how='left')\n",
    "\n",
    "nft_df.to_csv('nft_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Scrap Twitter accouts with Solenium (depreciated)\n",
    "\n",
    "twitter_followers = []\n",
    "twitter_following = []\n",
    "\n",
    "for twitter_name in df['Twitter']:\n",
    "    print('\\n')\n",
    "    print('=======================================')\n",
    "    print('Start with {0}'.format(twitter_name))\n",
    "    \n",
    "    # Build the twitter url based on the account name\n",
    "    url = 'https://mobile.twitter.com/{0}'.format(twitter_name)\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Find the number of followers and following on Twitter's account\n",
    "    delay = 5 # seconds\n",
    "    try:\n",
    "        # Try to find 'href' the name given by rarity tools on Twitter account\n",
    "        myElem = WebDriverWait(driver, delay).until(EC.presence_of_element_located((By.CSS_SELECTOR, 'a[href=\"/{0}/followers\"]'.format(twitter_name))))\n",
    "    except TimeoutException:\n",
    "        try:\n",
    "            # Try to find the body of the page\n",
    "            myElem = WebDriverWait(driver, delay).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
    "        except TimeoutException:\n",
    "            n_followers = 'None'\n",
    "            n_following = 'None'\n",
    "            print(\"Can't find the body of the page for account {0}\".format(twitter_name))\n",
    "        else:\n",
    "            # Find the real account name in the body of the page\n",
    "            tag = driver.find_element_by_tag_name('body')\n",
    "            idx = tag.text.lower().find(twitter_name)\n",
    "            if idx >= 0:\n",
    "                twitter_name = tag.text[idx:idx+len(twitter_name)]\n",
    "            try:\n",
    "                # Try to find 'href' with the name found in the body of the page\n",
    "                myElem = WebDriverWait(driver, delay).until(EC.presence_of_element_located((By.CSS_SELECTOR, 'a[href=\"/{0}/followers\"]'.format(twitter_name))))\n",
    "            except TimeoutException:\n",
    "                twitter_name = twitter_name.lower()\n",
    "                try:\n",
    "                    # Try with lower case\n",
    "                    myElem = WebDriverWait(driver, delay).until(EC.presence_of_element_located((By.CSS_SELECTOR, 'a[href=\"/{0}/followers\"]'.format(twitter_name))))\n",
    "                except TimeoutException:\n",
    "                    n_followers = 'None'\n",
    "                    n_following = 'None'\n",
    "                    print(\"Can't find twitter information for {0}\".format(twitter_name))\n",
    "                else:\n",
    "                    n_followers = driver.find_element_by_css_selector('a[href=\"/{0}/followers\"] > span > span'.format(twitter_name)).text\n",
    "                    n_following = driver.find_element_by_css_selector('a[href=\"/{0}/following\"] > span > span'.format(twitter_name)).text            \n",
    "                    print(\"{0} done with 'href' in lower case\".format(twitter_name))\n",
    "            else:\n",
    "                n_followers = driver.find_element_by_css_selector('a[href=\"/{0}/followers\"] > span > span'.format(twitter_name)).text\n",
    "                n_following = driver.find_element_by_css_selector('a[href=\"/{0}/following\"] > span > span'.format(twitter_name)).text                  \n",
    "                print(\"{0} done with 'href' found on Twitter account!\".format(twitter_name))\n",
    "    else:\n",
    "        n_followers = driver.find_element_by_css_selector('a[href=\"/{0}/followers\"] > span > span'.format(twitter_name)).text\n",
    "        n_following = driver.find_element_by_css_selector('a[href=\"/{0}/following\"] > span > span'.format(twitter_name)).text\n",
    "        print(\"{0} done!\".format(twitter_name))\n",
    "\n",
    "    # Store number of followers and following\n",
    "    twitter_followers.append(n_followers)\n",
    "    twitter_following.append(n_following)\n",
    "    print('=======================================')\n",
    "    \n",
    "# Close the selenium driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch Tweets for each NFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating list to append tweet data\n",
    "tweets_list = []\n",
    "\n",
    "# We are interested to get tweets from the last 7 days\n",
    "today = date.today().strftime(\"%Y-%m-%d\")\n",
    "last_month = (date.today() - timedelta(days=7)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Maximum tweets to scrap for each query\n",
    "max_tweets = 5000\n",
    "\n",
    "# Use TwitterSearchScraper to scrape data for each NFT\n",
    "for search_query in tqdm(nft_df['Twitter Profile']):\n",
    "    for i,tweet in enumerate(sntwitter.TwitterSearchScraper(\n",
    "        '{0} since:{1} until:{2}'.format(search_query, last_month, today)).get_items()):\n",
    "        if i > max_tweets:\n",
    "                break\n",
    "        tweets_list.append([search_query, tweet.date, tweet.id, \n",
    "                            tweet.content, tweet.user.username, tweet.user.created, \n",
    "                            tweet.user.description, tweet.user.favouritesCount, tweet.user.followersCount, \n",
    "                            tweet.user.friendsCount, tweet.user.listedCount, tweet.user.statusesCount, \n",
    "                            tweet.user.mediaCount, tweet.user.location, tweet.user.profileBannerUrl, \n",
    "                            tweet.user.profileImageUrl])\n",
    "\n",
    "# Creating a dataframe from the tweets list above\n",
    "tweets_df = pd.DataFrame(tweets_list, columns=['Twitter Profile', 'Datetime', 'Tweet Id', 'Text', 'Username',\n",
    "                                               'User Created', 'User Description', 'User Favourites Count', \n",
    "                                               'User Followers Count', 'User Friends Count', 'User Listed Count', \n",
    "                                               'User Statuses Count', 'User Media Count', 'User Location', \n",
    "                                               'User Profile Banner Url', 'User Profile Image Url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control if any of the NFTs has been recently mentionned by an influencer\n",
    "\n",
    "# Influencer list taken on: \n",
    "# - https://coinbound.io/top-nft-influencers/\n",
    "# - https://itsblockchain.com/top-15-nft-influencers-on-twitter-you-should-follow-right-now/\n",
    "\n",
    "influencers = ['garyvee', 'DeezeFi', 'farokh', 'mevcollector', 'beaniemax', 'RealmissNFT', \n",
    "               'CozomoMedici', 'peruggia_v', 'andrwwang', 'iamDCinvestor', 'punk4156',  'punk6529', \n",
    "               'TheTreeverse', 'KennethBosak', 'tsmith', 'Bosslogic', 'NFTLive']\n",
    "\n",
    "print('We found {0} influencers in the list'.format(len(tweets_df.loc[tweets_df['Twitter Profile'].isin(influencers)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store Twitter information\n",
    "tweets_df.to_csv('tweets.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store information about Google trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Article of reference: https://medium.com/geekculture/easily-gather-google-trends-data-in-python-22219cecd6fc\n",
    "\n",
    "# hl: host language\n",
    "# tz: timezone in minutes after UTC (positive only)\n",
    "# timeout: between 5 and 10 sec if the server is not responding\n",
    "pytrends = TrendReq(hl='en-US', tz=60, timeout=(5,10))\n",
    "\n",
    "# keyword list\n",
    "kw_list = [\"Nft\"]\n",
    "\n",
    "# cat: google trend category (https://github.com/pat310/google-trends-api/wiki/Google-Trends-Categories)\n",
    "# timeframe: 12 months ago from today\n",
    "# geo represents the country in two-digit ISO codes (defaults to worldwide if left blank)\n",
    "# gprop represents Google properties such as news, images, youtube etc. (defaults to web searches if left blank)\n",
    "pytrends.build_payload(kw_list, cat=0, timeframe='today 12-m', geo='', gprop='')\n",
    "\n",
    "# Interest over time\n",
    "df_interest_over_time = pytrends.interest_over_time()\n",
    "\n",
    "# Interest by Region\n",
    "df_interest_by_region = pytrends.interest_by_region()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store current trend\n",
    "df_interest_over_time.to_csv('google_trend_over_time.csv')\n",
    "df_interest_by_region.to_csv('google_trend_per_region.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
