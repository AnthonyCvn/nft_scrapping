{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining for NFTs\n",
    "\n",
    "- Scrap and store data from rarity tool website about trendy NFTs\n",
    "- Find influencers in NFT and spot if they speak about a particular NFT\n",
    "- Store information about Google trends\n",
    "\n",
    "Idea for implementation\n",
    "- When was the Twitter account created\n",
    "- sentiment analysis on Twitts\n",
    "- Volume\n",
    "- Launch date of the NFT\n",
    "- ETH value (if low buy more easily)\n",
    "\n",
    "Packages that need to be installed\n",
    "- !pip install pytrends\n",
    "- !pip3 install snscrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Progress bar library\n",
    "from tqdm import tqdm\n",
    "\n",
    "# We will use snscrape from github.com/JustAnotherArchivist/snscrape to get data from Twitter\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "from datetime import date, timedelta\n",
    "\n",
    "# We use pytrend to fetch information from Google trend\n",
    "from pytrends.request import TrendReq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrap and store data from rarity tool website about trendy NFTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import selenium webdriver and configure its options\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "options = Options()\n",
    "options.headless = False\n",
    "options.add_argument(\"--window-size=1920,1200\")\n",
    "\n",
    "driver = webdriver.Chrome(options=options, executable_path='./chromedriver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get driver for rarity tool website\n",
    "driver.get('https://rarity.tools/upcoming')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rarity tool page is ready!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Bricktopians',\n",
       " '3D avatar NFTs generated by AI',\n",
       " 'Discord',\n",
       " '@bricktopians',\n",
       " 'bricktopians.com',\n",
       " '0.08 ETH',\n",
       " '10,000 Total',\n",
       " 'Presale:',\n",
       " '1:30 pm (Europe/Berlin)',\n",
       " 'Yesterday',\n",
       " 'Sale:',\n",
       " '1:30 pm (Europe/Berlin)',\n",
       " 'Tuesday, November 23rd 2021']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wait until page is loaded\n",
    "delay = 10 # seconds\n",
    "try:\n",
    "    myElem = WebDriverWait(driver, delay).until(EC.presence_of_element_located((By.XPATH, \"//*[contains(@class,'text-left text-gray-800')]\")))\n",
    "    print(\"Rarity tool page is ready!\")\n",
    "except TimeoutException:\n",
    "    print(\"Loading took longer than {0} seconds\".format(delay))\n",
    "\n",
    "# Find NFT elements\n",
    "url_elements = driver.find_elements_by_xpath(\"//*[contains(@class,'text-left text-gray-800')]\")\n",
    "\n",
    "# Show first NFT\n",
    "url_elements[0].text.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with NFTs from rarity tool\n",
    "nft_list = []\n",
    "for element in url_elements:\n",
    "    nft_list.append(element.text.split('\\n'))\n",
    "\n",
    "df = pd.DataFrame(nft_list)\n",
    "\n",
    "# Close Selenium driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the name, description, website, volume, and twitter account\n",
    "df['Name'] = df[0]\n",
    "df['Description'] = df[1]\n",
    "df['Website'] = df[4]\n",
    "df['Twitter Profile'] = df[3].apply(lambda x: x[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the price tag\n",
    "def cleanPrice(x):\n",
    "    if 'eth' in x.lower():\n",
    "        return float(''.join([i for i in x if i.isnumeric() or i=='.']))\n",
    "    else:\n",
    "        return 0\n",
    "df['Price ETH'] = df[5].apply(cleanPrice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data on each Twitter profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 13/58 [00:17<01:01,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twitter profile ainsei.com is not a valid username\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 45/58 [00:58<00:17,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twitter profile coochycoopanda?s=21 is not a valid username\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58/58 [01:14<00:00,  1.29s/it]\n"
     ]
    }
   ],
   "source": [
    "# Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "profile_list = []\n",
    "\n",
    "# Use TwitterSearchScraper to scrape data for each NFT\n",
    "for twitter_name in tqdm(df['Twitter Profile']):\n",
    "    if sntwitter.TwitterUserScraper.is_valid_username(twitter_name):\n",
    "        try:\n",
    "            items = next(sntwitter.TwitterProfileScraper('{0}'.format(twitter_name), isUserId=False).get_items())\n",
    "        except AttributeError:\n",
    "            print('Twitter profile {0} not found'.format(twitter_name))\n",
    "        else:  \n",
    "            profile_list.append([twitter_name, items.user.created, items.user.description, \n",
    "                                 items.user.favouritesCount, items.user.followersCount, \n",
    "                                 items.user.friendsCount, items.user.listedCount, items.user.statusesCount, \n",
    "                                 items.user.mediaCount, items.user.location, items.user.profileBannerUrl, \n",
    "                                 items.user.profileImageUrl])\n",
    "    else:\n",
    "        print('Twitter profile {0} is not a valid username'.format(twitter_name))\n",
    "\n",
    "# Creating a dataframe from the tweets list above\n",
    "profiles_df = pd.DataFrame(profile_list, columns=['Twitter Profile','Twitter User Created', 'Twitter User Description', 'Twitter User Favourites Count', \n",
    "                                                  'Twitter User Followers Count', 'Twitter User Friends Count', 'Twitter User Listed Count', \n",
    "                                                  'Twitter User Statuses Count', 'Twitter User Media Count', 'Twitter User Location', \n",
    "                                                  'Twitter User Profile Banner Url', 'Twitter User Profile Image Url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge and save dataframe\n",
    "nft_df = df[['Twitter Profile', 'Name', 'Description', 'Website', 'Price ETH']].merge(profiles_df, on='Twitter Profile', how='left')\n",
    "\n",
    "nft_df.to_csv('nft_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Scrap Twitter accouts with Solenium (depreciated)\n",
    "\n",
    "twitter_followers = []\n",
    "twitter_following = []\n",
    "\n",
    "for twitter_name in df['Twitter']:\n",
    "    print('\\n')\n",
    "    print('=======================================')\n",
    "    print('Start with {0}'.format(twitter_name))\n",
    "    \n",
    "    # Build the twitter url based on the account name\n",
    "    url = 'https://mobile.twitter.com/{0}'.format(twitter_name)\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Find the number of followers and following on Twitter's account\n",
    "    delay = 5 # seconds\n",
    "    try:\n",
    "        # Try to find 'href' the name given by rarity tools on Twitter account\n",
    "        myElem = WebDriverWait(driver, delay).until(EC.presence_of_element_located((By.CSS_SELECTOR, 'a[href=\"/{0}/followers\"]'.format(twitter_name))))\n",
    "    except TimeoutException:\n",
    "        try:\n",
    "            # Try to find the body of the page\n",
    "            myElem = WebDriverWait(driver, delay).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
    "        except TimeoutException:\n",
    "            n_followers = 'None'\n",
    "            n_following = 'None'\n",
    "            print(\"Can't find the body of the page for account {0}\".format(twitter_name))\n",
    "        else:\n",
    "            # Find the real account name in the body of the page\n",
    "            tag = driver.find_element_by_tag_name('body')\n",
    "            idx = tag.text.lower().find(twitter_name)\n",
    "            if idx >= 0:\n",
    "                twitter_name = tag.text[idx:idx+len(twitter_name)]\n",
    "            try:\n",
    "                # Try to find 'href' with the name found in the body of the page\n",
    "                myElem = WebDriverWait(driver, delay).until(EC.presence_of_element_located((By.CSS_SELECTOR, 'a[href=\"/{0}/followers\"]'.format(twitter_name))))\n",
    "            except TimeoutException:\n",
    "                twitter_name = twitter_name.lower()\n",
    "                try:\n",
    "                    # Try with lower case\n",
    "                    myElem = WebDriverWait(driver, delay).until(EC.presence_of_element_located((By.CSS_SELECTOR, 'a[href=\"/{0}/followers\"]'.format(twitter_name))))\n",
    "                except TimeoutException:\n",
    "                    n_followers = 'None'\n",
    "                    n_following = 'None'\n",
    "                    print(\"Can't find twitter information for {0}\".format(twitter_name))\n",
    "                else:\n",
    "                    n_followers = driver.find_element_by_css_selector('a[href=\"/{0}/followers\"] > span > span'.format(twitter_name)).text\n",
    "                    n_following = driver.find_element_by_css_selector('a[href=\"/{0}/following\"] > span > span'.format(twitter_name)).text            \n",
    "                    print(\"{0} done with 'href' in lower case\".format(twitter_name))\n",
    "            else:\n",
    "                n_followers = driver.find_element_by_css_selector('a[href=\"/{0}/followers\"] > span > span'.format(twitter_name)).text\n",
    "                n_following = driver.find_element_by_css_selector('a[href=\"/{0}/following\"] > span > span'.format(twitter_name)).text                  \n",
    "                print(\"{0} done with 'href' found on Twitter account!\".format(twitter_name))\n",
    "    else:\n",
    "        n_followers = driver.find_element_by_css_selector('a[href=\"/{0}/followers\"] > span > span'.format(twitter_name)).text\n",
    "        n_following = driver.find_element_by_css_selector('a[href=\"/{0}/following\"] > span > span'.format(twitter_name)).text\n",
    "        print(\"{0} done!\".format(twitter_name))\n",
    "\n",
    "    # Store number of followers and following\n",
    "    twitter_followers.append(n_followers)\n",
    "    twitter_following.append(n_following)\n",
    "    print('=======================================')\n",
    "    \n",
    "# Close the selenium driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch Tweets for each NFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58/58 [26:33<00:00, 27.47s/it]  \n"
     ]
    }
   ],
   "source": [
    "# Creating list to append tweet data\n",
    "tweets_list = []\n",
    "\n",
    "# We are interested to get tweets from the last 7 days\n",
    "today = date.today().strftime(\"%Y-%m-%d\")\n",
    "last_month = (date.today() - timedelta(days=7)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Maximum tweets to scrap for each query\n",
    "max_tweets = 5000\n",
    "\n",
    "# Use TwitterSearchScraper to scrape data for each NFT\n",
    "for search_query in tqdm(nft_df['Twitter Profile']):\n",
    "    for i,tweet in enumerate(sntwitter.TwitterSearchScraper(\n",
    "        '{0} since:{1} until:{2}'.format(search_query, last_month, today)).get_items()):\n",
    "        if i > max_tweets:\n",
    "                break\n",
    "        tweets_list.append([search_query, tweet.date, tweet.id, \n",
    "                            tweet.content, tweet.user.username, tweet.user.created, \n",
    "                            tweet.user.description, tweet.user.favouritesCount, tweet.user.followersCount, \n",
    "                            tweet.user.friendsCount, tweet.user.listedCount, tweet.user.statusesCount, \n",
    "                            tweet.user.mediaCount, tweet.user.location, tweet.user.profileBannerUrl, \n",
    "                            tweet.user.profileImageUrl])\n",
    "\n",
    "# Creating a dataframe from the tweets list above\n",
    "tweets_df = pd.DataFrame(tweets_list, columns=['Twitter Profile', 'Datetime', 'Tweet Id', 'Text', 'Username',\n",
    "                                               'User Created', 'User Description', 'User Favourites Count', \n",
    "                                               'User Followers Count', 'User Friends Count', 'User Listed Count', \n",
    "                                               'User Statuses Count', 'User Media Count', 'User Location', \n",
    "                                               'User Profile Banner Url', 'User Profile Image Url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We found 0 influencers in the list\n"
     ]
    }
   ],
   "source": [
    "# Control if any of the NFTs has been recently mentionned by an influencer\n",
    "\n",
    "# Influencer list taken on: \n",
    "# - https://coinbound.io/top-nft-influencers/\n",
    "# - https://itsblockchain.com/top-15-nft-influencers-on-twitter-you-should-follow-right-now/\n",
    "\n",
    "influencers = ['garyvee', 'DeezeFi', 'farokh', 'mevcollector', 'beaniemax', 'RealmissNFT', \n",
    "               'CozomoMedici', 'peruggia_v', 'andrwwang', 'iamDCinvestor', 'punk4156',  'punk6529', \n",
    "               'TheTreeverse', 'KennethBosak', 'tsmith', 'Bosslogic', 'NFTLive']\n",
    "\n",
    "print('We found {0} influencers in the list'.format(len(tweets_df.loc[tweets_df['Twitter Profile'].isin(influencers)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store Twitter information\n",
    "tweets_df.to_csv('tweets.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store information about Google trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Article of reference: https://medium.com/geekculture/easily-gather-google-trends-data-in-python-22219cecd6fc\n",
    "\n",
    "# hl: host language\n",
    "# tz: timezone in minutes after UTC (positive only)\n",
    "# timeout: between 5 and 10 sec if the server is not responding\n",
    "pytrends = TrendReq(hl='en-US', tz=60, timeout=(5,10))\n",
    "\n",
    "# keyword list\n",
    "kw_list = [\"Nft\"]\n",
    "\n",
    "# cat: google trend category (https://github.com/pat310/google-trends-api/wiki/Google-Trends-Categories)\n",
    "# timeframe: 12 months ago from today\n",
    "# geo represents the country in two-digit ISO codes (defaults to worldwide if left blank)\n",
    "# gprop represents Google properties such as news, images, youtube etc. (defaults to web searches if left blank)\n",
    "pytrends.build_payload(kw_list, cat=0, timeframe='today 12-m', geo='', gprop='')\n",
    "\n",
    "# Interest over time\n",
    "df_interest_over_time = pytrends.interest_over_time()\n",
    "\n",
    "# Interest by Region\n",
    "df_interest_by_region = pytrends.interest_by_region()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store current trend\n",
    "df_interest_over_time.to_csv('google_trend_over_time.csv')\n",
    "df_interest_by_region.to_csv('google_trend_per_region.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
